{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.0\n",
      "1.16.0\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "print(dash.__version__)\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_core_components as dcc\n",
    "print(dcc.__version__)\n",
    "import dash_table\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gharbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gharbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gharbi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from coclust.coclustering import CoclustMod\n",
    "from coclust.clustering import SphericalKmeans\n",
    "from coclust.coclustering import CoclustSpecMod\n",
    "from scipy.sparse import coo_matrix\n",
    "from coclust.visualization import plot_cluster_sizes, plot_cluster_top_terms\n",
    "from coclust.visualization import (plot_cluster_top_terms,get_term_graph, plot_convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_preprocessing(df_corpus):\n",
    "    j=0\n",
    "    for i in df_corpus['Abstract']:\n",
    "        if i==\"null\":\n",
    "            df_corpus = df_corpus.drop(labels=j)\n",
    "    j=j+1\n",
    "    return df_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    # suppression de pontuation et caracteres numériques\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    # en lettres minuscules \n",
    "    text = text.lower()\n",
    "    # tokenisation : prendre chaque mot a sa case\n",
    "    text = word_tokenize(text)\n",
    "    # suppression des stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    # Lemmatiser les mots : exemple : Screening => screen , investigated => investigate\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = [lemma.lemmatize(word=w, pos='v') for w in text]\n",
    "    # supprimer les mots de taille inférieur à 2\n",
    "    text= [i for i in text if len(i) > 2]\n",
    "    # reconvertir en String\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "def search(term, rmax):\n",
    "    Entrez.email = ''\n",
    "    handle = Entrez.esearch(db='pubmed', # DB\n",
    "                            sort='relevance',  # tri par relevance\n",
    "                            retmax=rmax, # combien d'articles\n",
    "                            retmode='xml', \n",
    "                            term=term) # mot clé\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "    \n",
    "def parse_authors(id_list):\n",
    "    h = Entrez.efetch(db='pubmed', id=id_list, rettype='medline', retmode='text')\n",
    "    records = Medline.parse(h)\n",
    "    authors_list = []\n",
    "    for record in records:\n",
    "        au = record.get('AU', '?')\n",
    "        for a in au: \n",
    "            if a not in authors_list:\n",
    "                authors_list.append(a)\n",
    "    return authors_list\n",
    "\n",
    "def article_date(source):\n",
    "    try :\n",
    "        return source[0]['Year'] + '-' + source[0]['Month'] + '-' + source[0]['Day']\n",
    "    except : return np.nan\n",
    "    \n",
    "def recup(num,terme):\n",
    "    title_list = []\n",
    "    abst_list = []\n",
    "    date_list = []\n",
    "    results = search(terme, num)\n",
    "    #   IDs :\n",
    "    id_list = results['IdList']\n",
    "\n",
    "    papers = fetch_details(id_list)\n",
    "    for i, paper in enumerate(papers['PubmedArticle']):\n",
    "\n",
    "    #       titles : \n",
    "        title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "        print(\"{}) {}\".format(i+1, title))\n",
    "        title_list.append(title)\n",
    "\n",
    "    #       Abstracts : \n",
    "        try:\n",
    "            abst = paper['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "            abst = str(abst).strip(\"['']\")\n",
    "            abst_list.append(abst)\n",
    "        except:\n",
    "            abst = \"null\"\n",
    "            abst_list.append(abst)\n",
    "\n",
    "    #       Dates : \n",
    "        d = article_date(paper['MedlineCitation']['Article']['ArticleDate'])\n",
    "        date_list.append(d)\n",
    "\n",
    "    #   Authors:\n",
    "    auth_list = []\n",
    "    for id in id_list:\n",
    "        auth = parse_authors(id)\n",
    "        auth_list.append(auth)\n",
    "\n",
    "    for i, abst in enumerate(abst_list):\n",
    "        abst_list[i] = preprocessing(abst)\n",
    "\n",
    "    dict_corpus = {'Article ID' : id_list, 'Titre' : title_list, 'Auteurs': auth_list, 'Date': date_list, 'Abstract' : abst_list}\n",
    "    df_corpus = pd.DataFrame(dict_corpus)\n",
    "\n",
    "    for i,a in enumerate(df_corpus['Auteurs']):\n",
    "        df_corpus['Auteurs'][i] = ', '.join(a)\n",
    "    df_corpus = second_preprocessing(df_corpus)\n",
    "    return df_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotXMLError",
     "evalue": "Failed to parse the XML data (XML declaration not found). Please make sure that the input data are in XML format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotXMLError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-78077032be34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cancer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-fcaf43820233>\u001b[0m in \u001b[0;36mrecup\u001b[1;34m(num, terme)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mabst_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mdate_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;31m#   IDs :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mid_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IdList'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fcaf43820233>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(term, rmax)\u001b[0m\n\u001b[0;32m     26\u001b[0m                             \u001b[0mretmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xml'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                             term=term) # mot clé\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEntrez\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\Entrez\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(handle, validate, escape)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[0mhandler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m     \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Bio\\Entrez\\Parser.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[1;31m# We did not see the initial <!xml declaration, so probably\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;31m# the input data is not in XML format.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotXMLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XML declaration not found\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotXMLError\u001b[0m: Failed to parse the XML data (XML declaration not found). Please make sure that the input data are in XML format."
     ]
    }
   ],
   "source": [
    "df = recup(10,'cancer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation_text(text,mindf,maxdf):    \n",
    "    vectorizer = TfidfVectorizer(min_df=mindf,\n",
    "                               max_df=maxdf,\n",
    "                               max_features=None,\n",
    "                               stop_words='english').fit(text)\n",
    "    \n",
    "    X = vectorizer.fit_transform(text)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return X, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text, features = vectorisation_text(df['Abstract'],0.05,0.9)\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 6\n",
    "def lda(number_of_topics,text_lda):\n",
    "    model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "    model.fit(text_lda)\n",
    "    # Transform the TF-IDF: nmf_features\n",
    "    lda_features = model.transform(text_lda)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda(6,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords = 5\n",
    "topics = display_topics(model, features, nwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method : \n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=0).fit(data).inertia_)     \n",
    "    return iters, sse\n",
    "\n",
    "\n",
    "x =[]\n",
    "y=[]\n",
    "x, y = find_optimal_clusters(text, 10)\n",
    "x = list(x)\n",
    "y= list(y)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "elbow = go.Figure(data=[go.Scatter(x=x, y=y)],layout = go.Layout(\n",
    "        title=\"Elbow\",\n",
    "        height=400,\n",
    "        width=500\n",
    "        ))\n",
    "elbow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(text_kmeans,number):\n",
    "    clusters = MiniBatchKMeans(n_clusters=number, init_size=1024, batch_size=2048, random_state=20).fit_predict(text_kmeans)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_pca(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=True)\n",
    "    \n",
    "    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n",
    "    tsne = TSNE().fit_transform(PCA().fit_transform(data[max_items,:].todense()))\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n",
    "    label_subset = labels[max_items]\n",
    "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    \n",
    "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
    "    ax[1].set_title('TSNE Cluster Plot')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "def fig_to_uri(in_fig, close_all=True, **save_args):\n",
    "    # type: (plt.Figure) -> str\n",
    "    \"\"\"\n",
    "    Save a figure as a URI\n",
    "    :param in_fig:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    out_img = BytesIO()\n",
    "    in_fig.savefig(out_img, format='png', **save_args)\n",
    "    if close_all:\n",
    "        in_fig.clf()\n",
    "        plt.close('all')\n",
    "    out_img.seek(0)  # rewind file\n",
    "    encoded = base64.b64encode(out_img.read()).decode(\"ascii\").replace(\"\\n\", \"\")\n",
    "    return \"data:image/png;base64,{}\".format(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour récupérer le nombre de clusters optimale grace a la modularité(plus grande valeur)\n",
    "from coclust.visualization import plot_max_modularities\n",
    "from coclust.evaluation.internal import best_modularity_partition\n",
    "\n",
    "clusters_range = range(2, 10 )\n",
    "model, modularities = best_modularity_partition(text, clusters_range, n_rand_init=1)\n",
    "# plot the modularities over the range of cluster numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coclust.coclustering import CoclustMod\n",
    "from coclust.clustering import SphericalKmeans\n",
    "from coclust.coclustering import CoclustSpecMod\n",
    "from scipy.sparse import coo_matrix\n",
    "from coclust.visualization import plot_cluster_sizes, plot_cluster_top_terms\n",
    "from coclust.visualization import (plot_cluster_top_terms,get_term_graph, plot_convergence)\n",
    "n_cluster = 8\n",
    "model_mod = CoclustMod(n_clusters = n_cluster, random_state = 0) \n",
    "model_mod.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_top_terms(in_data, all_terms, nb_top_terms, model):\n",
    "    \"\"\"Plot the top terms for each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_data : numpy array or scipy sparse matrix, shape=(n_samples, n_features)\n",
    "    all_terms: list of string\n",
    "        list of all terms from the original data set\n",
    "    nb_top_terms: int\n",
    "        number of top terms to be displayed per cluster\n",
    "    model: :class:`coclust.coclustering.BaseDiagonalCoclust`\n",
    "        a co-clustering model\n",
    "\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> plot_cluster_top_terms(in_data, all_terms, nb_top_terms, model)\n",
    "\n",
    "    .. plot::\n",
    "\n",
    "        from coclust.visualization import plot_cluster_top_terms\n",
    "        from coclust.io.data_loading import load_doc_term_data\n",
    "        from coclust.evaluation.internal import best_modularity_partition\n",
    "\n",
    "        path = '../../../datasets/classic3_coclustFormat.mat'\n",
    "        doc_term_data = load_doc_term_data(path)\n",
    "\n",
    "        min_cluster_nbr = 2\n",
    "        max_cluster_nbr = 9\n",
    "        range_n_clusters = range(min_cluster_nbr, (max_cluster_nbr + 1))\n",
    "\n",
    "        best_coclustMod_model, _ = \\\n",
    "            best_modularity_partition(doc_term_data['doc_term_matrix'],\n",
    "                                      range_n_clusters, 1)\n",
    "        n_terms = 10\n",
    "        plot_cluster_top_terms(doc_term_data['doc_term_matrix'],\n",
    "                               doc_term_data['term_labels'],\n",
    "                               n_terms,\n",
    "                               best_coclustMod_model)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if all_terms is None:\n",
    "        logger.warning(\"Term labels cannot be found. Use input argument \"\n",
    "                       \"'term_labels_filepath' in function \"\n",
    "                       \"'load_doc_term_data' if term labels are available.\")\n",
    "        return\n",
    "\n",
    "    x_label = \"number of occurences\"\n",
    "    plt.subplots(figsize=(15, 15))\n",
    "    plt.subplots_adjust(hspace=0.200)\n",
    "    plt.suptitle(\"      Top %d terms\" % nb_top_terms, size=15)\n",
    "    number_of_subplots = model.n_clusters\n",
    "\n",
    "    for i, v in enumerate(range(number_of_subplots)):\n",
    "        # Get the row/col indices corresponding to the given cluster\n",
    "        row_indices, col_indices = model.get_indices(v)\n",
    "        # Get the submatrix corresponding to the given cluster\n",
    "        cluster = model.get_submatrix(in_data, v)\n",
    "        # Count the number of each term\n",
    "        p = cluster.sum(0)\n",
    "        t = p.getA().flatten()\n",
    "        # Obtain all term names for the given cluster\n",
    "        tmp_terms = np.array(all_terms)[col_indices]\n",
    "        # Get the first n terms\n",
    "        max_indices = t.argsort()[::-1][:nb_top_terms]\n",
    "\n",
    "        pos = np.arange(nb_top_terms)\n",
    "\n",
    "        v = v + 1\n",
    "        f, ax1 = plt.subplot(number_of_subplots, 1, v)\n",
    "        ax1.barh(pos, t[max_indices][::-1])\n",
    "        ax1.set_title(\"Cluster %d (%d terms)\" % (v, len(col_indices)), size=11)\n",
    "\n",
    "        plt.yticks(.4 + pos, tmp_terms[max_indices][::-1], size=9.5)\n",
    "        plt.xlabel(x_label, size=9)\n",
    "        plt.margins(y=0.05)\n",
    "        #_remove_ticks()\n",
    "        plt.tick_params(axis='both', which='both', bottom='on', top='off',\n",
    "                        right='off', left='off')\n",
    "\n",
    "    # Tight layout often produces nice results\n",
    "    # but requires the title to be spaced accordingly\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    return f\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__,external_stylesheets=[dbc.themes.BOOTSTRAP],prevent_initial_callbacks=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = dbc.Row(\n",
    "    [\n",
    "        dbc.Col(\n",
    "                dbc.Input(id=\"num_max\",type=\"number\", placeholder=\"nombre d'articles\", className=\"mr-3\"),className=\"col-4\"),\n",
    "        dbc.Col(dbc.Input(id =\"word\", type=\"search\", placeholder=\"Covid-19\", className=\"mr-3\"),className=\"col-4\"),\n",
    "        dbc.Col(\n",
    "            dbc.Button(\"recuperer\",id=\"submitt_search\", color=\"success\", className=\"ml-2\"),\n",
    "            width=\"auto\",\n",
    "        ),\n",
    "    ],\n",
    "    no_gutters=True,\n",
    "    className=\"mr-auto flex-nowrap mt-3 mt-md-0\",\n",
    "    align=\"center\",\n",
    "    id=\"navbar_out\"\n",
    ")\n",
    "\n",
    "sidebar = html.Div(\n",
    "    id = \"links\",\n",
    "    className=\"sidebare\",\n",
    "    children =[\n",
    "        dbc.Nav(\n",
    "            [\n",
    "                dbc.Button(\"details fetching\", href=\"/page-1\", active=\"exact\"),\n",
    "                dbc.Button(\"K-means\", href=\"/page-2\", active=\"exact\"),\n",
    "                dbc.Button(\"LDA\", href=\"/page-3\", active=\"exact\"),\n",
    "                dbc.Button(\"Co-clustering\", href=\"/page-4\", active=\"exact\")\n",
    "            ],\n",
    "            vertical=True,\n",
    "            pills=True,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "graphe = dash_table.DataTable(\n",
    "    id='table',\n",
    "    style_cell={\n",
    "        'whiteSpace': 'normal',\n",
    "        'height': 'auto',\n",
    "    },\n",
    "    style_table={\n",
    "            'width': 950,\n",
    "            'overflowY': 'auto',\n",
    "            'overflowX': 'auto',\n",
    "            'height': 540},\n",
    "    columns=[{\"name\": i, \"id\": i} for i in df.columns],\n",
    "    data=df.to_dict('records'),\n",
    "    export_format=\"csv\",\n",
    "\n",
    ")\n",
    "information_gen = dbc.Row(\n",
    "\n",
    "\n",
    "    className=\"row\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation = html.Div(\n",
    "    className=\"content\",\n",
    "    children=[\n",
    "        graphe,\n",
    "        information_gen,\n",
    "        \n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navbar = dbc.Navbar(\n",
    "    [\n",
    "        html.A(\n",
    "            # Use row and col to control vertical alignment of logo / brand\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dbc.NavbarBrand(\"Corpus médical\", className=\"ml-2\")),\n",
    "                ],\n",
    "                align=\"center\",\n",
    "                no_gutters=True,\n",
    "            ),\n",
    "            href=\"https://plot.ly\",\n",
    "        ),\n",
    "        dbc.NavbarToggler(id=\"navbar-toggler\"),\n",
    "        dbc.Collapse(search_bar, id=\"navbar-collapse\", navbar=True),\n",
    "        #no_gutters=True,\n",
    "    ],\n",
    "    color=\"dark\",\n",
    "    dark=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_1_layout = html.Div(\n",
    "    className=\"CONTENT_STYLE\",\n",
    "                  \n",
    "                 children = [\n",
    "                     visualisation\n",
    "                 ]\n",
    "                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_2_layout =html.Div(\n",
    "        dbc.Row([\n",
    "        dbc.Col(dcc.Graph(figure = elbow),className=\"col-4\"),\n",
    "        dbc.Col([\n",
    "            html.P(\"choose the number of clusters \"),\n",
    "            \n",
    "            dcc.Slider(\n",
    "            id='box_size',\n",
    "            min=1,\n",
    "            max=10,\n",
    "            value=4,\n",
    "            step=1,\n",
    "            marks=list(range(0, 10)),\n",
    "        ),\n",
    "         html.Div([html.Img(id = 'cur_plot', src = '')],\n",
    "                 id='plot_div')\n",
    "        ],className=\"col-8\")\n",
    "        \n",
    "\n",
    "    ],\n",
    "    className=\"mr-auto flex-nowrap mt-3 mt-md-0\",\n",
    "),\n",
    "    id=\"layout2\"\n",
    ") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_4_layout =html.Div(\n",
    "        dbc.Row([\n",
    "        dbc.Col(html.Div([html.Img(id = 'cur_plotco', src = '')],\n",
    "                 id='plot_div'),className=\"col-4\"),\n",
    "        dbc.Col([\n",
    "            html.P(\"choose the number of clusters \"),\n",
    "            \n",
    "            dcc.Slider(\n",
    "            id='box_sizecoc',\n",
    "            min=1,\n",
    "            max=10,\n",
    "            value=4,\n",
    "            step=1,\n",
    "            marks=list(range(0, 10)),\n",
    "        ),\n",
    "         html.Div([html.Img(id = 'cur_plotcoc', src = '')],\n",
    "                 id='plot_div')\n",
    "        ],className=\"col-8\")\n",
    "        \n",
    "\n",
    "    ],\n",
    "    className=\"mr-auto flex-nowrap mt-3 mt-md-0\",\n",
    "),\n",
    "    id=\"layout4\"\n",
    ") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_3_layout =html.Div(\n",
    "    \n",
    "    id=\"layout3\",\n",
    "    className=\"content\",\n",
    "    children =[\n",
    "     dbc.Col([\n",
    "                html.P(\"choose the number of clusters \"),\n",
    "            \n",
    "            dcc.Slider(\n",
    "            id='box_sizecoc',\n",
    "            min=1,\n",
    "            max=10,\n",
    "            value=4,\n",
    "            step=1,\n",
    "            marks=list(range(0, 10)),\n",
    "        ),\n",
    "            \n",
    "     ]),\n",
    "    dash_table.DataTable(\n",
    "    id='table',\n",
    "    style_cell={\n",
    "        'whiteSpace': 'normal',\n",
    "        'height': 'auto',\n",
    "    },\n",
    "    style_table={\n",
    "            'width': 950,\n",
    "            'overflowY': 'auto',\n",
    "            'overflowX': 'auto',\n",
    "            'height': 540},\n",
    "    columns=[{\"name\": i, \"id\": i} for i in topics.columns],\n",
    "    data=topics.to_dict('records'),\n",
    "    export_format=\"csv\",\n",
    "\n",
    ")]\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.layout = html.Div([dcc.Store(id='storage'),dcc.Location(id=\"url\"), navbar,sidebar,html.Div(id =\"page-content\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output(component_id='cur_plot', component_property='src'),\n",
    "    [Input(component_id = 'box_size', component_property='value')]\n",
    ")\n",
    "def update_graph( n_val):\n",
    "    clusters = kmeans(text, n_val)\n",
    "    fig = plot_tsne_pca(text, clusters)\n",
    "    out_url = fig_to_uri(fig)\n",
    "    return out_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output(component_id='cur_plotcoc', component_property='src'),\n",
    "    [Input(component_id = 'box_sizecoc', component_property='value')]\n",
    ")\n",
    "def update_graph2( n_val):\n",
    "    n_cluster = 5\n",
    "    model_mod = CoclustMod(n_clusters = n_cluster, random_state = 0) \n",
    "    model_mod.fit(text)\n",
    "    fig = plot_cluster_top_terms(text,features,5,model_mod)\n",
    "    out_url = fig_to_uri(fig)\n",
    "    return out_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add callback for toggling the collapse on small screens\n",
    "@app.callback(\n",
    "    Output(\"navbar-collapse\", \"is_open\"),\n",
    "    [Input(\"navbar-toggler\", \"n_clicks\")],\n",
    "    [State(\"navbar-collapse\", \"is_open\")]\n",
    ")\n",
    "def toggle_navbar_collapse(n,is_open):\n",
    "    if n:\n",
    "        return not is_open\n",
    "    return is_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output(\"table\", \"data\"),\n",
    "    [Input(\"submitt_search\",\"n_clicks\")],\n",
    "    [State('num_max', 'value'),\n",
    "     State('word', 'value'),]\n",
    ")\n",
    "def fetch(n_clicks,num_max,word):\n",
    "    df= recup(num_max,word)\n",
    "    return df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(dash.dependencies.Output('page-content', 'children'),\n",
    "              [dash.dependencies.Input('url', 'pathname')])\n",
    "def display_page(pathname):\n",
    "    if pathname == '/page-1':\n",
    "        return page_1_layout\n",
    "    elif pathname == '/page-2':\n",
    "        return page_2_layout\n",
    "    elif pathname == '/page-3':\n",
    "        return page_3_layout\n",
    "    elif pathname == '/page-4':\n",
    "        return page_4_layout\n",
    "    else:\n",
    "        return \"404 not found\"\n",
    "    # You could also return a 404 \"URL not found\" page here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.config.suppress_callback_exceptions = True\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
